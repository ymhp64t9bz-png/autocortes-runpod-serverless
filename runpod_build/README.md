# ğŸ¬ AutoCortes - RunPod Serverless

Sistema completo de processamento de vÃ­deo com IA para criaÃ§Ã£o automÃ¡tica de cortes virais.

## ğŸš€ Features

- âœ… TranscriÃ§Ã£o automÃ¡tica com Whisper
- âœ… GeraÃ§Ã£o de tÃ­tulos virais com Llama 3
- âœ… AnÃ¡lise de segmentos com DeepSeek R1
- âœ… RenderizaÃ§Ã£o com NVENC (GPU)
- âœ… Anti-shadowban automÃ¡tico
- âœ… Suporte a templates personalizados
- âœ… Processamento em lote
- âœ… Serverless ready

## ğŸ“‹ Requisitos

### GPU
- NVIDIA GPU com CUDA 11.8+
- MÃ­nimo 8GB VRAM (recomendado 12GB+)

### Modelos
- Whisper Medium (~1.5GB)
- Llama 3 8B Q4 (~4.5GB)
- DeepSeek R1 8B Q4 (~4.5GB)

## ğŸ› ï¸ InstalaÃ§Ã£o Local

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/autocortes-runpod.git
cd autocortes-runpod

# Crie ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale dependÃªncias
pip install -r requirements.txt

# Teste localmente
python handler.py
```

## â˜ï¸ Deploy no RunPod Serverless

### 1. Preparar Imagem Docker

```bash
# Build
docker build -t autocortes-serverless .

# Tag para DockerHub
docker tag autocortes-serverless seu-usuario/autocortes-serverless:latest

# Push
docker push seu-usuario/autocortes-serverless:latest
```

### 2. Configurar no RunPod

1. Acesse [RunPod Serverless](https://www.runpod.io/serverless)
2. Crie novo endpoint
3. Configure:
   - **Docker Image**: `seu-usuario/autocortes-serverless:latest`
   - **GPU**: A6000 ou superior
   - **Container Disk**: 20GB
   - **Volume**: 50GB (para modelos)

### 3. Testar Endpoint

```python
import requests

endpoint_url = "https://api.runpod.ai/v2/YOUR_ENDPOINT_ID/run"
headers = {
    "Authorization": "Bearer YOUR_API_KEY"
}

# Processar vÃ­deo
payload = {
    "input": {
        "operation": "process_video",
        "video_url": "https://example.com/video.mp4",
        "anime_name": "Naruto",
        "mode": "auto",
        "config": {
            "font_size": 70,
            "text_color": "#FFD700",
            "stroke_color": "#000000",
            "stroke_width": 6,
            "pos_vertical": 0.15,
            "anti_shadowban": True,
            "usar_ia": True
        }
    }
}

response = requests.post(endpoint_url, json=payload, headers=headers)
print(response.json())
```

## ğŸ“Š OperaÃ§Ãµes Suportadas

### 1. Processar VÃ­deo Completo

```json
{
  "input": {
    "operation": "process_video",
    "video_url": "https://...",
    "anime_name": "Nome do Anime",
    "mode": "auto",
    "config": {...}
  }
}
```

### 2. Transcrever Ãudio

```json
{
  "input": {
    "operation": "transcribe_audio",
    "audio_url": "https://..."
  }
}
```

### 3. Gerar TÃ­tulo

```json
{
  "input": {
    "operation": "generate_title",
    "anime_name": "Naruto",
    "dialogue": "Texto do diÃ¡logo..."
  }
}
```

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RunPod Serverless               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  handler.py                             â”‚
â”‚    â”œâ”€ initialize_models()               â”‚
â”‚    â”œâ”€ process_video()                   â”‚
â”‚    â”œâ”€ process_auto_mode()               â”‚
â”‚    â””â”€ process_manual_mode()             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /app/src/                              â”‚
â”‚    â”œâ”€ core/                             â”‚
â”‚    â”‚   â”œâ”€ ai_services/                  â”‚
â”‚    â”‚   â”‚   â””â”€ local_ai_service.py       â”‚
â”‚    â”‚   â””â”€ webapp.py                     â”‚
â”‚    â”œâ”€ modules/                          â”‚
â”‚    â”‚   â”œâ”€ AnimeCut/                     â”‚
â”‚    â”‚   â”œâ”€ KwaiCut/                      â”‚
â”‚    â”‚   â””â”€ VIRAL_PRO/                    â”‚
â”‚    â””â”€ fontes/                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Models (cached)                        â”‚
â”‚    â”œâ”€ Whisper Medium                    â”‚
â”‚    â”œâ”€ Llama 3 8B                        â”‚
â”‚    â””â”€ DeepSeek R1 8B                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# .env
RUNPOD_API_KEY=your_key_here
GEMINI_API_KEY=your_gemini_key  # Fallback opcional
```

## ğŸ“ˆ Performance

- **TranscriÃ§Ã£o**: ~2-3min para 10min de vÃ­deo
- **GeraÃ§Ã£o de TÃ­tulo**: ~5-10s por tÃ­tulo
- **RenderizaÃ§Ã£o**: ~30s por corte de 60s

## ğŸ› Troubleshooting

### Erro de VRAM

```bash
# Reduzir tamanho do modelo Whisper
# Em handler.py, mudar de 'medium' para 'small'
```

### Timeout

```bash
# Aumentar timeout no RunPod
# Settings > Timeout > 600s
```

## ğŸ“ LicenÃ§a

ProprietÃ¡rio - AutoCortes Team

## ğŸ¤ Suporte

- Email: suporte@autocortes.com
- Discord: [Link]
- Docs: [Link]

---

**Desenvolvido com â¤ï¸ por AutoCortes Team**
