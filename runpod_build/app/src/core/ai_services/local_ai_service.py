# -*- coding: utf-8 -*-
import os
import gc
import logging
import re
import sys

# Configura√ß√£o de Logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Vari√°veis Globais para Inst√¢ncias (Singleton)
_WHISPER_INSTANCE = None
_LLAMA_INSTANCE = None

# ==================== DETEC√á√ÉO DE AMBIENTE ====================

def is_running_in_colab():
    """Detecta se o c√≥digo est√° rodando no Google Colab."""
    return 'COLAB_GPU' in os.environ or 'GCS_READ_BUFFERS' in os.environ

def get_model_path(model_name):
    """Retorna o caminho do modelo baseado no ambiente."""
    if is_running_in_colab():
        # Caminho Colab
        colab_path = f'/content/{model_name}'
        logger.info(f"[PATH] Ambiente Colab detectado. Usando: {colab_path}")
        return colab_path
    else:
        # Caminho Local (Windows)
        # core/ai_services/local_ai_service.py -> core -> AutoCortes -> models
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        local_path = os.path.join(base_dir, 'models', model_name)
        logger.info(f"[PATH] Ambiente Local detectado. Usando: {local_path}")
        return local_path

# Caminhos dos Modelos (Din√¢micos)
MODEL_PATH_LLAMA = get_model_path('llama-3-8b-instruct.Q4_K_M.gguf')
MODEL_SIZE_WHISPER = "medium"

def _clean_memory():
    """For√ßa a limpeza da VRAM e RAM."""
    gc.collect()
    try:
        import torch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except ImportError:
        pass
    logger.info("[MEMORIA] Limpeza for√ßada conclu√≠da.")

# ==================== WHISPER (STT) ====================

def load_whisper_model():
    global _WHISPER_INSTANCE
    if _WHISPER_INSTANCE is None:
        logger.info(f"[WHISPER] Carregando modelo '{MODEL_SIZE_WHISPER}'...")
        try:
            import whisper
            import torch
            
            # For√ßa uso da GPU se dispon√≠vel
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"[WHISPER] Usando device: {device}")
            
            _WHISPER_INSTANCE = whisper.load_model(MODEL_SIZE_WHISPER, device=device)
            logger.info("[WHISPER] Modelo carregado com sucesso.")
            
            # Mostra VRAM usada
            if torch.cuda.is_available():
                vram_usada = torch.cuda.memory_allocated(0) / (1024**3)
                logger.info(f"[WHISPER] VRAM usada: {vram_usada:.2f}GB")
                
        except ImportError:
            logger.error("[WHISPER] Biblioteca 'openai-whisper' n√£o instalada.")
            _WHISPER_INSTANCE = None
        except Exception as e:
            logger.error(f"[WHISPER] Erro ao carregar: {e}")
            _WHISPER_INSTANCE = None

def unload_whisper_model():
    global _WHISPER_INSTANCE
    if _WHISPER_INSTANCE is not None:
        logger.info("[WHISPER] Descarregando modelo...")
        del _WHISPER_INSTANCE
        _WHISPER_INSTANCE = None
        _clean_memory()

def transcribe_audio_local(audio_path):
    """
    Fluxo Serializado: Load -> Transcribe -> Unload
    """
    # Converte para caminho absoluto longo (resolve problema de caminhos curtos do Windows)
    audio_path = os.path.abspath(audio_path)
    
    if not os.path.exists(audio_path):
        logger.error(f"[WHISPER] Arquivo n√£o encontrado: {audio_path}")
        return None

    try:
        load_whisper_model()
        if _WHISPER_INSTANCE is None:
            return None

        # Limpa cache CUDA antes de transcrever
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info(f"[WHISPER] Cache CUDA limpo. VRAM livre: {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB")
        except:
            pass

        logger.info(f"[WHISPER] Transcrevendo: {audio_path}")
        logger.info(f"[WHISPER] Tamanho do arquivo: {os.path.getsize(audio_path) / 1024**2:.2f}MB")
        
        # Detecta FP16
        try:
            import torch
            use_fp16 = torch.cuda.is_available()
        except:
            use_fp16 = False
        
        logger.info(f"[WHISPER] Iniciando transcri√ß√£o (FP16={use_fp16})...")
        result = _WHISPER_INSTANCE.transcribe(
            audio_path, 
            fp16=use_fp16,
            verbose=False,
            language='pt'
        )
        logger.info(f"[WHISPER] Transcri√ß√£o conclu√≠da! Texto: {len(result.get('text', ''))} caracteres")
        return result
    except Exception as e:
        logger.error(f"[WHISPER] Erro na transcri√ß√£o: {e}")
        import traceback
        logger.error(f"[WHISPER] Traceback: {traceback.format_exc()}")
        return None
    finally:
        unload_whisper_model()

def transcribe_audio_batch(audio_path):
    """
    Transcreve √°udio SEM descarregar o modelo.
    Use esta fun√ß√£o durante renderiza√ß√£o em lote.
    Chame manually_unload_whisper() ao final do lote.
    """
    # Converte para caminho absoluto longo
    audio_path = os.path.abspath(audio_path)
    
    if not os.path.exists(audio_path):
        logger.error(f"[WHISPER] Arquivo n√£o encontrado: {audio_path}")
        return None

    try:
        # Carrega o modelo se ainda n√£o estiver carregado
        if _WHISPER_INSTANCE is None:
            load_whisper_model()
        
        if _WHISPER_INSTANCE is None:
            return None

        # Limpa cache CUDA antes de transcrever
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info(f"[WHISPER] Cache CUDA limpo. VRAM livre: {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB")
        except:
            pass

        logger.info(f"[WHISPER] Transcrevendo: {audio_path}")
        logger.info(f"[WHISPER] Tamanho do arquivo: {os.path.getsize(audio_path) / 1024**2:.2f}MB")
        
        # Detecta FP16
        try:
            import torch
            use_fp16 = torch.cuda.is_available()
        except:
            use_fp16 = False
        
        logger.info(f"[WHISPER] Iniciando transcri√ß√£o (FP16={use_fp16})...")
        result = _WHISPER_INSTANCE.transcribe(
            audio_path, 
            fp16=use_fp16,
            verbose=False,  # Desativa logs verbosos do Whisper
            language='pt'   # Define portugu√™s para acelerar
        )
        logger.info(f"[WHISPER] Transcri√ß√£o conclu√≠da! Texto: {len(result.get('text', ''))} caracteres")
        return result
    except Exception as e:
        logger.error(f"[WHISPER] Erro na transcri√ß√£o: {e}")
        import traceback
        logger.error(f"[WHISPER] Traceback: {traceback.format_exc()}")
        return None
    # N√ÉO descarrega o modelo aqui!

def manually_unload_whisper():
    """Descarrega manualmente o modelo Whisper ap√≥s processamento em lote."""
    unload_whisper_model()

# ==================== LLAMA 3 8B INSTRUCT (LLM) ====================

def load_llama_model():
    global _LLAMA_INSTANCE
    if _LLAMA_INSTANCE is None:
        if not os.path.exists(MODEL_PATH_LLAMA):
            logger.error(f"[LLAMA] Modelo n√£o encontrado em: {MODEL_PATH_LLAMA}")
            return
        
        logger.info("[LLAMA] Carregando Llama 3 8B Instruct...")
        try:
            from llama_cpp import Llama
            _LLAMA_INSTANCE = Llama(
                model_path=MODEL_PATH_LLAMA,
                n_ctx=8192,
                n_gpu_layers=0,  # 0 = CPU apenas (libera VRAM para Whisper e renderiza√ß√£o)
                verbose=False
            )
            logger.info("[LLAMA] Modelo carregado (CPU).")
        except ImportError:
            logger.error("[LLAMA] Biblioteca 'llama-cpp-python' n√£o instalada.")
            _LLAMA_INSTANCE = None
        except Exception as e:
            logger.error(f"[LLAMA] Erro ao carregar: {e}")
            _LLAMA_INSTANCE = None

def unload_llama_model():
    global _LLAMA_INSTANCE
    if _LLAMA_INSTANCE is not None:
        logger.info("[LLAMA] Descarregando modelo...")
        del _LLAMA_INSTANCE
        _LLAMA_INSTANCE = None
        _clean_memory()

def generate_viral_title_local(anime_name, dialogue):
    """Gera t√≠tulo viral baseado no di√°logo."""
    try:
        load_llama_model()
        if _LLAMA_INSTANCE is None:
            return f"{anime_name} - CENA √âPICA"

        system_prompt = (
            "Voc√™ √© um especialista em marketing viral. Analise o di√°logo e crie 3 t√≠tulos curtos (max 5 palavras) "
            "e impactantes para TikTok. Use gatilhos de curiosidade. Responda apenas com os t√≠tulos."
        )
        user_prompt = f"Anime: {anime_name}\nDi√°logo: \"{dialogue[:1000]}\"\n\nT√≠tulos Virais:"
        
        output = _LLAMA_INSTANCE.create_chat_completion(
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            max_tokens=100, temperature=0.7
        )
        
        full_response = output['choices'][0]['message']['content'].strip()
        titles = [t.strip().replace('"', '').replace('-', '').strip() for t in full_response.split('\n') if t.strip()]
        return titles[0] if titles else f"{anime_name} - MOMENTO VIRAL"
    except Exception as e:
        logger.error(f"[DEEPSEEK] Erro t√≠tulo: {e}")
        return f"{anime_name} - CENA DE A√á√ÉO"
    finally:
        unload_llama_model()

# ==================== LIMPEZA DE RESPOSTA LLAMA 3 ====================

def clean_llama_response(raw_text):
    """
    Limpeza simples para Llama 3 (n√£o gera racioc√≠nio com prompt correto).
    
    Args:
        raw_text: Resposta crua do modelo
        
    Returns:
        Texto limpo
    """
    # 1. Remove aspas
    clean_text = raw_text.replace('"', '').replace("'", '').strip()
    
    # 2. Remove marcadores comuns
    markers = ['Title:', 'T√≠tulo:', '**', '*', ':', '-']
    for marker in markers:
        clean_text = clean_text.replace(marker, '')
    
    # 3. Pega primeira linha
    if '\n' in clean_text:
        clean_text = clean_text.split('\n')[0].strip()
    
    # 4. Remove v√≠rgulas iniciais
    clean_text = clean_text.lstrip(',').strip()
    
    return clean_text.strip()

def generate_viral_title_batch(anime_name, dialogue):
    """
    Gera t√≠tulo viral SEM descarregar o modelo.
    Use esta fun√ß√£o durante renderiza√ß√£o em lote.
    Chame manually_unload_llama() ao final do lote.
    """
    try:
        # Carrega o modelo se ainda n√£o estiver carregado
        if _LLAMA_INSTANCE is None:
            load_llama_model()
        
        if _LLAMA_INSTANCE is None:
            return f"{anime_name.upper()} CENA √âPICA"

        # Prompt anti-racioc√≠nio (espec√≠fico para Llama 3)
        system_prompt = (
            "You are a title generator. "
            "Respond ONLY with the title. "
            "NO explanations. NO reasoning. NO thinking process. "
            "JUST the title in Portuguese, uppercase, 4-9 words."
        )
        
        user_prompt = (
            f"Anime: {anime_name}\n"
            f"Dialogue: \"{dialogue[:300]}\"\n\n"
            f"Create viral TikTok title (Portuguese, UPPERCASE, 4-9 words):"
        )
        
        logger.info(f"[LLAMA] Gerando t√≠tulo para {anime_name}...")
        
        # Gera t√≠tulo com par√¢metros restritivos
        output = _LLAMA_INSTANCE.create_chat_completion(
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=30,  # Limita para evitar racioc√≠nio
            temperature=0.7,
            stop=["\n", ".", "Explanation:", "Reasoning:"],  # Para se come√ßar a raciocinar
            repeat_penalty=1.1  # Evita repeti√ß√£o
        )
        
        raw_response = output['choices'][0]['message']['content'].strip()
        logger.info(f"[LLAMA] Resposta crua: {raw_response[:100]}...")
        
        # Limpa resposta (simples para Llama 3)
        titulo = clean_llama_response(raw_response)
        
        # Se ap√≥s limpeza estiver vazio, usa fallback baseado no di√°logo
        if not titulo or len(titulo) < 10:
            logger.warning("[LLAMA] T√≠tulo vazio ap√≥s limpeza. Usando fallback.")
            palavras = dialogue[:300].upper().split()
            stop_words = ['O', 'A', 'E', 'DE', 'DA', 'DO', 'EM', 'NA', 'NO', 'QUE', 'PARA', 'COM', 'POR', 'SEM', '√â', 'S√ÉO']
            palavras_chave = [p for p in palavras if len(p) > 3 and p not in stop_words][:7]
            if len(palavras_chave) >= 4:
                titulo = " ".join(palavras_chave[:7])
            else:
                titulo = " ".join(palavras_chave) + f" EM {anime_name.upper()}"
        
        # Converte para mai√∫sculas
        titulo = titulo.upper()
        
        # Limita a 80 caracteres
        if len(titulo) > 80:
            titulo = titulo[:80].rsplit(' ', 1)[0].strip()
        
        # Valida n√∫mero de palavras (4-9)
        palavras = titulo.split()
        if len(palavras) < 4:
            titulo = f"{titulo} EM {anime_name.upper()}"
        elif len(palavras) > 9:
            titulo = ' '.join(palavras[:9])
        
        logger.info(f"[LLAMA] T√≠tulo final: {titulo}")
        return titulo
            
    except Exception as e:
        logger.error(f"[LLAMA] Erro ao gerar t√≠tulo: {e}")
        import traceback
        logger.error(f"[LLAMA] Traceback: {traceback.format_exc()}")
        # Fallback baseado no di√°logo
        try:
            palavras = dialogue[:300].upper().split()
            stop_words = ['O', 'A', 'E', 'DE', 'DA', 'DO', 'EM', 'NA', 'NO', 'QUE', 'PARA', 'COM', 'POR', 'SEM', '√â', 'S√ÉO']
            palavras_chave = [p for p in palavras if len(p) > 3 and p not in stop_words][:7]
            if len(palavras_chave) >= 4:
                return " ".join(palavras_chave[:7])
            else:
                return " ".join(palavras_chave) + f" EM {anime_name.upper()}"
        except:
            return f"{anime_name.upper()} CENA √âPICA"
    # N√ÉO descarrega o modelo aqui!

def manually_unload_llama():
    """Descarrega manualmente o modelo DeepSeek ap√≥s processamento em lote."""
    unload_llama_model()

def analyze_viral_segments_deepseek(full_transcript_text, duration_total):
    """
    Analisa a transcri√ß√£o completa e identifica segmentos virais (60s-180s).
    SEM LIMITA√á√ïES - Permite que o DeepSeek trabalhe at√© o final.
    """
    try:
        load_llama_model()
        if _LLAMA_INSTANCE is None:
            logger.error("[DEEPSEEK] Modelo n√£o carregado para an√°lise de segmentos.")
            return []

        logger.info("[DEEPSEEK] Analisando roteiro para cortes autom√°ticos...")
        logger.info("[DEEPSEEK] Modo SEM LIMITA√á√ïES ativado - DeepSeek trabalhar√° at√© concluir")
        
        # Prompt direto
        system_prompt = (
            "Voc√™ √© um editor de v√≠deo especialista. Analise o roteiro completo e identifique os melhores momentos para clipes virais. "
            "Retorne APENAS uma lista de timestamps no formato [INICIO-FIM]. "
            "Exemplo: [0-120], [300-450], [600-750]"
        )
        
        # Envia a transcri√ß√£o COMPLETA (sem limitar a 8000 caracteres)
        user_prompt = (
            f"Dura√ß√£o total do v√≠deo: {duration_total} segundos.\n\n"
            f"Identifique 3-5 momentos virais (cada um com 60-180 segundos de dura√ß√£o).\n\n"
            f"Roteiro completo:\n{full_transcript_text}\n\n"
            f"Retorne APENAS os timestamps no formato [INICIO-FIM], separados por v√≠rgula:"
        )
        
        logger.info(f"[DEEPSEEK] Enviando {len(full_transcript_text)} caracteres de transcri√ß√£o...")
        
        # Limita tamanho da transcri√ß√£o para evitar travamento (CPU tem limite)
        max_chars = 12000  # Limite seguro para CPU
        if len(full_transcript_text) > max_chars:
            logger.warning(f"[DEEPSEEK] Transcri√ß√£o muito longa ({len(full_transcript_text)} chars). Limitando a {max_chars} chars.")
            transcript_to_send = full_transcript_text[:max_chars]
        else:
            transcript_to_send = full_transcript_text
        
        # Atualiza prompt com transcri√ß√£o limitada
        user_prompt = (
            f"Dura√ß√£o total do v√≠deo: {duration_total} segundos.\n\n"
            f"Identifique 3-5 momentos virais (cada um com 60-180 segundos de dura√ß√£o).\n\n"
            f"Roteiro (primeiros {len(transcript_to_send)} caracteres):\n{transcript_to_send}\n\n"
            f"Retorne APENAS os timestamps no formato [INICIO-FIM], separados por v√≠rgula:"
        )
        
        logger.info(f"[DEEPSEEK] Processando {len(transcript_to_send)} caracteres...")
        logger.info(f"[DEEPSEEK] Aguarde - DeepSeek analisando (pode levar 30-60s)...")
        logger.info(f"[DEEPSEEK] Iniciando processamento em tempo real...")
        
        import time
        start_time = time.time()
        
        # Com streaming para mostrar progresso
        try:
            logger.info(f"[DEEPSEEK] ‚è≥ Gerando resposta (streaming ativado)...")
            
            response_text = ""
            token_count = 0
            
            # Cria stream
            stream = _LLAMA_INSTANCE.create_chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt}, 
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.3,
                stream=True  # Ativa streaming
            )
            
            # Processa tokens em tempo real
            for chunk in stream:
                if 'choices' in chunk and len(chunk['choices']) > 0:
                    delta = chunk['choices'][0].get('delta', {})
                    if 'content' in delta:
                        token_text = delta['content']
                        response_text += token_text
                        token_count += 1
                        
                        # Log a cada 10 tokens
                        if token_count % 10 == 0:
                            elapsed = time.time() - start_time
                            logger.info(f"[DEEPSEEK] üìù {token_count} tokens gerados ({elapsed:.1f}s)...")
            
            elapsed_total = time.time() - start_time
            logger.info(f"[DEEPSEEK] ‚úÖ Resposta completa! {token_count} tokens em {elapsed_total:.1f}s")
            
            response = response_text.strip()
            
        except Exception as stream_error:
            # Fallback para modo n√£o-streaming se falhar
            logger.warning(f"[DEEPSEEK] Streaming falhou: {stream_error}. Usando modo padr√£o...")
            
            output = _LLAMA_INSTANCE.create_chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt}, 
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            response = output['choices'][0]['message']['content'].strip()
            elapsed_total = time.time() - start_time
            logger.info(f"[DEEPSEEK] Resposta recebida em {elapsed_total:.1f}s")
        
        logger.info(f"[DEEPSEEK] Resposta completa recebida ({len(response)} caracteres)")
        logger.info(f"[DEEPSEEK] Primeiros 500 caracteres: {response[:500]}...")
        
        # Parser robusto de timestamps
        segments = []
        matches = re.findall(r'\[(\d+)-(\d+)\]', response)
        
        if not matches:
            logger.warning("[DEEPSEEK] Nenhum timestamp no formato [X-Y] encontrado. Tentando padr√£o alternativo...")
            # Tenta padr√£o sem colchetes, mas apenas n√∫meros grandes (timestamps reais)
            all_matches = re.findall(r'(\d+)\s*-\s*(\d+)', response)
            matches = [(s, e) for s, e in all_matches if int(e) - int(s) >= 30]
        
        if matches:
            logger.info(f"[DEEPSEEK] {len(matches)} timestamps encontrados")
            
            for start, end in matches:
                s, e = int(start), int(end)
                duration = e - s
                
                # Valida dura√ß√£o
                if duration >= 60 and duration <= 180:
                    if s < duration_total:
                        e = min(e, duration_total)
                        segments.append({'start': s, 'end': e})
                        logger.info(f"[DEEPSEEK] ‚úì Segmento v√°lido: {s}s-{e}s (dura√ß√£o: {duration}s)")
                else:
                    logger.warning(f"[DEEPSEEK] ‚úó Segmento ignorado (dura√ß√£o {duration}s fora do range 60-180s): {s}-{e}")
        else:
            logger.warning("[DEEPSEEK] Nenhum timestamp v√°lido encontrado na resposta")
        
        # Fallback autom√°tico se necess√°rio
        if not segments and duration_total > 60:
            logger.info("[DEEPSEEK] Criando segmentos autom√°ticos como fallback...")
            num_segments = min(5, int(duration_total / 120))
            segment_duration = duration_total / num_segments if num_segments > 0 else 120
            
            for i in range(num_segments):
                start = int(i * segment_duration)
                end = int(min(start + 120, duration_total))
                if end - start >= 60:
                    segments.append({'start': start, 'end': end})
                    logger.info(f"[DEEPSEEK] ‚úì Segmento autom√°tico: {start}s-{end}s")
        
        # Remove duplicatas (mesmo start e end)
        unique_segments = []
        seen = set()
        for seg in segments:
            key = (seg['start'], seg['end'])
            if key not in seen:
                seen.add(key)
                unique_segments.append(seg)
        
        # Limita a 5 segmentos (pega os primeiros)
        if len(unique_segments) > 5:
            logger.warning(f"[DEEPSEEK] {len(unique_segments)} segmentos encontrados. Limitando a 5.")
            unique_segments = unique_segments[:5]
        
        logger.info(f"[DEEPSEEK] Total de {len(unique_segments)} segmentos identificados")
        return unique_segments
    except Exception as e:
        logger.error(f"[DEEPSEEK] Erro na an√°lise de segmentos: {e}")
        import traceback
        logger.error(f"[DEEPSEEK] Traceback: {traceback.format_exc()}")
        return []
    # N√ÉO descarrega o modelo aqui - ser√° reutilizado para gerar t√≠tulos
